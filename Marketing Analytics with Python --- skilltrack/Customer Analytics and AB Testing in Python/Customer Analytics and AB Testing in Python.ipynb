{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67422491",
   "metadata": {},
   "source": [
    "## Key Performance Indicators: Measuring Business Success"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67ea3cc",
   "metadata": {},
   "source": [
    "This chapter provides a brief introduction to the content that will be covered throughout the course before transitioning into a discussion of Key Performance Indicators or KPIs. You'll learn how to identify and define meaningful KPIs through a combination of critical thinking and leveraging Python tools. These techniques are all presented in a highly practical and generalizable way. Ultimately these topics serve as the core foundation for the A/B testing discussion that follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c64789",
   "metadata": {},
   "source": [
    "### Identifying and understanding KPIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c32033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading & examining our data\n",
    "\n",
    "# Import pandas \n",
    "import pandas as pd\n",
    "\n",
    "# Load the customer_data\n",
    "customer_data = pd.read_csv('customer_data.csv')\n",
    "\n",
    "# Load the app_purchases\n",
    "app_purchases = pd.read_csv('inapp_purchases.csv')\n",
    "\n",
    "# Print the columns of customer data\n",
    "print(customer_data.columns)\n",
    "\n",
    "# Print the columns of app_purchases\n",
    "print(app_purchases.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceca33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging on different sets of fields\n",
    "\n",
    "# Merge on the 'uid' field\n",
    "uid_combined_data = app_purchases.merge(customer_data, on=['uid'], how='inner')\n",
    "\n",
    "# Examine the results \n",
    "print(uid_combined_data.head())\n",
    "print(len(uid_combined_data))\n",
    "\n",
    "# Merge on the 'uid' and 'date' field\n",
    "uid_date_combined_data = app_purchases.merge(customer_data, on=['uid', 'date'], how='inner')\n",
    "\n",
    "# Examine the results \n",
    "print(uid_date_combined_data.head())\n",
    "print(len(uid_date_combined_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e432bc85",
   "metadata": {},
   "source": [
    "### Exploratory analysis of KPIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6208b07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practicing aggregations\n",
    "\n",
    "# Calculate the mean purchase price \n",
    "purchase_price_mean = purchase_data.price.agg('mean')\n",
    "\n",
    "# Examine the output \n",
    "print(purchase_price_mean)\n",
    "\n",
    "# Calculate the mean and median purchase price \n",
    "purchase_price_summary = purchase_data.price.agg(['mean', 'median'])\n",
    "\n",
    "# Examine the output \n",
    "print(purchase_price_summary)\n",
    "\n",
    "# Calculate the mean and median of price and age\n",
    "purchase_summary = purchase_data.agg({'price': ['mean', 'median'], 'age': ['mean', 'median']})\n",
    "\n",
    "# Examine the output \n",
    "print(purchase_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04651fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping & aggregating\n",
    "\n",
    "# Group the data \n",
    "grouped_purchase_data = purchase_data.groupby(by = ['device', 'gender'])\n",
    "\n",
    "# Aggregate the data\n",
    "purchase_summary = grouped_purchase_data.agg({'price': ['mean', 'median', 'std']})\n",
    "\n",
    "# Examine the results\n",
    "print(purchase_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00e56dc",
   "metadata": {},
   "source": [
    "### Calculating KPIs - a practical example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bf33e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute max_purchase_date \n",
    "max_purchase_date = current_date - timedelta(days=28)\n",
    "\n",
    "# Filter to only include users who registered before our max date\n",
    "purchase_data_filt = purchase_data[purchase_data.reg_date < max_purchase_date]\n",
    "\n",
    "# Filter to contain only purchases within the first 28 days of registration\n",
    "purchase_data_filt = purchase_data_filt[(purchase_data_filt.date <=\n",
    "                                         purchase_data_filt.reg_date + \n",
    "                                         timedelta(days=28))]\n",
    "\n",
    "# Output the mean price paid per purchase\n",
    "print(purchase_data_filt.price.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bf25f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average purchase price by cohort\n",
    "\n",
    "# Set the max registration date to be one month before today\n",
    "max_reg_date = current_date - timedelta(days=28)\n",
    "\n",
    "# Find the month 1 values:\n",
    "month1 = np.where((purchase_data.reg_date < max_reg_date) &\n",
    "                    (purchase_data.date < purchase_data.reg_date + timedelta(days=28)),\n",
    "                  purchase_data.price, \n",
    "                  np.NaN)\n",
    "                 \n",
    "# Update the value in the DataFrame \n",
    "purchase_data['month1'] = month1\n",
    "\n",
    "# Group the data by gender and device \n",
    "purchase_data_upd = purchase_data.groupby(by=['gender', 'device'], as_index=False)\n",
    "\n",
    "# Aggregate the month1 and price data \n",
    "purchase_summary = purchase_data_upd.agg(\n",
    "                        {'month1': ['mean', 'median'],\n",
    "                        'price': ['mean', 'median']})\n",
    "\n",
    "# Examine the results \n",
    "print(purchase_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4645c255",
   "metadata": {},
   "source": [
    "## Exploring and Visualizing Customer Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc6fcac",
   "metadata": {},
   "source": [
    "This chapter teaches you how to visualize, manipulate, and explore KPIs as they change over time. Through a variety of examples, you'll learn how to work with datetime objects to calculate metrics per unit time. Then we move to the techniques for how to graph different segments of data, and apply various smoothing functions to reveal hidden trends. Finally we walk through a complete example of how to pinpoint issues through exploratory data analysis of customer data. Throughout this chapter various functions are introduced and explained in a highly generalizable way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe9d138",
   "metadata": {},
   "source": [
    "### Working with time series data in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67833857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing dates\n",
    "\n",
    "# Provide the correct format for the date\n",
    "date_data_one = pd.to_datetime(date_data_one, format=\"%A %B %d, %Y\")\n",
    "print(date_data_one)\n",
    "\n",
    "# Provide the correct format for the date\n",
    "date_data_two = pd.to_datetime(date_data_two, format=\"%Y-%m-%d\")\n",
    "print(date_data_two)\n",
    "\n",
    "# Provide the correct format for the date\n",
    "date_data_three = pd.to_datetime(date_data_three, format=\"%m/%d/%Y\")\n",
    "print(date_data_three)\n",
    "\n",
    "# Provide the correct format for the date\n",
    "date_data_four = pd.to_datetime(date_data_four, format=\"%Y %B %d %H:%M\")\n",
    "print(date_data_four)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a1636c",
   "metadata": {},
   "source": [
    "### Creating time series graphs with matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc0c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting time series data\n",
    "\n",
    "# Group the data and aggregate first_week_purchases\n",
    "user_purchases = user_purchases.groupby(by=['reg_date', 'uid']).agg({'first_week_purchases': ['sum']})\n",
    "\n",
    "# Reset the indexes\n",
    "user_purchases.columns = user_purchases.columns.droplevel(level=1)\n",
    "user_purchases.reset_index(inplace=True)\n",
    "\n",
    "# Find the average number of purchases per day by first-week users\n",
    "user_purchases = user_purchases.groupby(by=['reg_date']).agg({'first_week_purchases': ['mean']})\n",
    "user_purchases.columns = user_purchases.columns.droplevel(level=1)\n",
    "user_purchases.reset_index(inplace=True)\n",
    "\n",
    "# Plot the results\n",
    "user_purchases.plot(x='reg_date', y='first_week_purchases')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2860572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivoting our data\n",
    "\n",
    "# Pivot the data\n",
    "country_pivot = pd.pivot_table(user_purchases_country, values=['first_week_purchases'], columns=['country'], index=['reg_date'])\n",
    "print(country_pivot.head())\n",
    "\n",
    "# Pivot the data\n",
    "device_pivot = pd.pivot_table(user_purchases_device, values=['first_week_purchases'], columns=['device'], index=['reg_date'])\n",
    "print(device_pivot.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f273b508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining the different cohorts\n",
    "\n",
    "# Plot the average first week purchases for each country by registration date\n",
    "country_pivot.plot(x='reg_date', y=['USA', 'CAN', 'FRA', 'BRA', 'TUR', 'DEU'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot the average first week purchases for each device by registration date\n",
    "device_pivot.plot(x='reg_date', y=['and', 'iOS'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1628e0b8",
   "metadata": {},
   "source": [
    "### Understanding and visualizing trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613dabb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonality and moving averages\n",
    "\n",
    "# Compute 7_day_rev\n",
    "daily_revenue['7_day_rev'] = daily_revenue.revenue.rolling(window=7,center=False).mean()\n",
    "\n",
    "# Compute 28_day_rev\n",
    "daily_revenue['28_day_rev'] = daily_revenue.revenue.rolling(window=28,center=False).mean()\n",
    "    \n",
    "# Compute 365_day_rev\n",
    "daily_revenue['365_day_rev'] = daily_revenue.revenue.rolling(window=365,center=False).mean()\n",
    "    \n",
    "# Plot date, and revenue, along with the 3 rolling functions (in order)    \n",
    "daily_revenue.plot(x='date', y=['revenue', '7_day_rev', '28_day_rev', '365_day_rev', ])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e81842f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential rolling average & over/under smoothing\n",
    "\n",
    "# Calculate 'small_scale'\n",
    "daily_revenue['small_scale'] = daily_revenue.revenue.ewm(span=10).mean()\n",
    "\n",
    "# Calculate 'medium_scale'\n",
    "daily_revenue['medium_scale'] = daily_revenue.revenue.ewm(span=100).mean()\n",
    "\n",
    "# Calculate 'large_scale'\n",
    "daily_revenue['large_scale'] = daily_revenue.revenue.ewm(span=500).mean()\n",
    "\n",
    "# Plot 'date' on the x-axis and, our three averages and 'revenue'\n",
    "# on the y-axis\n",
    "daily_revenue.plot(x = 'date', y =['revenue', 'small_scale', 'medium_scale', 'large_scale'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11ea340",
   "metadata": {},
   "source": [
    "### Events and releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe70f85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing user spending\n",
    "\n",
    "# Pivot user_revenue\n",
    "pivoted_data = pd.pivot_table(user_revenue, values ='revenue', columns=['device', 'gender'], index='month')\n",
    "pivoted_data = pivoted_data[1:(len(pivoted_data) -1 )]\n",
    "\n",
    "# Create and show the plot\n",
    "pivoted_data.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d46bfdf",
   "metadata": {},
   "source": [
    "## Introduction to A/B testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f66f82",
   "metadata": {},
   "source": [
    "In this chapter you will dive fully into A/B testing. You will learn the mathematics and knowledge needed to design and successfully plan an A/B test from determining an experimental unit to finding how large a sample size is needed. Accompanying this will be an introduction to the functions and code needed to calculate the various quantities associated with a statistical test of this type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715f3ab1",
   "metadata": {},
   "source": [
    "### Initial A/B test design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5fe88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental units: Revenue per user day\n",
    "\n",
    "# Extract the 'day'; value from the timestamp\n",
    "purchase_data.date = purchase_data.date.dt.floor('d')\n",
    "\n",
    "# Replace the NaN price values with 0 \n",
    "purchase_data.price = np.where(np.isnan(purchase_data.price), 0, purchase_data.price)\n",
    "\n",
    "# Aggregate the data by 'uid' & 'date'\n",
    "purchase_data_agg = purchase_data.groupby(by=['uid', 'date'], as_index=False)\n",
    "revenue_user_day = purchase_data_agg.sum()\n",
    "\n",
    "# Calculate the final average\n",
    "revenue_user_day = revenue_user_day.price.mean()\n",
    "print(revenue_user_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc8bd3b",
   "metadata": {},
   "source": [
    "### Preparing to run an A/B test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2a26a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion rate sensitivities\n",
    "\n",
    "# Merge and group the datasets\n",
    "purchase_data = demographics_data.merge(paywall_views,  how='inner', on=['uid'])\n",
    "purchase_data.date = purchase_data.date.dt.floor('d')\n",
    "\n",
    "# Group and aggregate our combined dataset \n",
    "daily_purchase_data = purchase_data.groupby(by=['date'], as_index=False)\n",
    "daily_purchase_data = daily_purchase_data.agg({'purchase': ['sum', 'count']})\n",
    "\n",
    "# Find the mean of each field and then multiply by 1000 to scale the result\n",
    "daily_purchases = daily_purchase_data.purchase['sum'].mean()\n",
    "daily_paywall_views = daily_purchase_data.purchase['count'].mean()\n",
    "daily_purchases = daily_purchases * 1000\n",
    "daily_paywall_views = daily_paywall_views * 1000\n",
    "\n",
    "print(daily_purchases)\n",
    "print(daily_paywall_views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8272b0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity\n",
    "\n",
    "small_sensitivity = 0.1 \n",
    "\n",
    "# Find the conversion rate when increased by the percentage of the sensitivity above\n",
    "small_conversion_rate = conversion_rate * (1 + small_sensitivity) \n",
    "\n",
    "# Apply the new conversion rate to find how many more users per day that translates to\n",
    "small_purchasers = daily_paywall_views * small_conversion_rate\n",
    "\n",
    "# Subtract the initial daily_purcahsers number from this new value to see the lift\n",
    "purchaser_lift = small_purchasers - daily_purchases\n",
    "\n",
    "print(small_conversion_rate)\n",
    "print(small_purchasers)\n",
    "print(purchaser_lift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fa93e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_sensitivity = 0.2\n",
    "\n",
    "# Find the conversion rate when increased by the percentage of the sensitivity above\n",
    "medium_conversion_rate = conversion_rate * (1 + medium_sensitivity) \n",
    "\n",
    "# Apply the new conversion rate to find how many more users per day that translates to\n",
    "medium_purchasers = daily_paywall_views * medium_conversion_rate\n",
    "\n",
    "# Subtract the initial daily_purcahsers number from this new value to see the lift\n",
    "purchaser_lift = medium_purchasers - daily_purchases\n",
    "\n",
    "print(medium_conversion_rate)\n",
    "print(medium_purchasers)\n",
    "print(purchaser_lift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cd390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_sensitivity = 0.5\n",
    "\n",
    "# Find the conversion rate lift with the sensitivity above\n",
    "large_conversion_rate = conversion_rate * (1 + large_sensitivity)\n",
    "\n",
    "# Find how many more users per day that translates to\n",
    "large_purchasers = daily_paywall_views * large_conversion_rate\n",
    "purchaser_lift = large_purchasers - daily_purchases\n",
    "\n",
    "print(large_conversion_rate)\n",
    "print(large_purchasers)\n",
    "print(purchaser_lift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9176a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard error\n",
    "\n",
    "# Find the number of paywall views \n",
    "n = purchase_data.purchase.count()\n",
    "\n",
    "# Calculate the quantitiy \"v\"\n",
    "v = conversion_rate * (1 - conversion_rate) \n",
    "\n",
    "# Calculate the variance and standard error of the estimate\n",
    "var = v / n \n",
    "se = var**0.5\n",
    "\n",
    "print(var)\n",
    "print(se)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
