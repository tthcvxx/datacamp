{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f416d32",
   "metadata": {},
   "source": [
    "## Cohort analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f487e34",
   "metadata": {},
   "source": [
    "In this first chapter, you will learn about cohorts and how to analyze them. You will create your own customer cohorts, get some metrics and visualize your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1237acc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign daily acquisition cohort\n",
    "\n",
    "# Define a function that will parse the date\n",
    "def get_day(x): return dt.datetime(x.year, x.month, x.day) \n",
    "\n",
    "# Create InvoiceDay column\n",
    "online['InvoiceDay'] = online['InvoiceDate'].apply(get_day) \n",
    "\n",
    "# Group by CustomerID and select the InvoiceDay value\n",
    "grouping = online.groupby('CustomerID')['InvoiceDay'] \n",
    "\n",
    "# Assign a minimum InvoiceDay value to the dataset\n",
    "online['CohortDay'] = grouping.transform('min')\n",
    "\n",
    "# View the top 5 rows\n",
    "print(online.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742c1d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract integer values from data\n",
    "\n",
    "def get_date_int(df, column):\n",
    "    year = df[column].dt.year\n",
    "    month = df[column].dt.month\n",
    "    day = df[column].dt.day\n",
    "    return year, month, day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47549ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate time offset in days - part 1\n",
    "\n",
    "# Get the integers for date parts from the `InvoiceDay` column\n",
    "invoice_year, invoice_month, invoice_day = get_date_int(online, 'InvoiceDay')\n",
    "\n",
    "# Get the integers for date parts from the `CohortDay` column\n",
    "cohort_year, cohort_month, cohort_day = get_date_int(online, 'CohortDay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581cbc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate time offset in days - part 2\n",
    "\n",
    "# Calculate difference in years\n",
    "years_diff = invoice_year - cohort_year\n",
    "\n",
    "# Calculate difference in months\n",
    "months_diff = invoice_month - cohort_month\n",
    "\n",
    "# Calculate difference in days\n",
    "days_diff = invoice_day - cohort_day\n",
    "\n",
    "# Extract the difference in days from all previous values\n",
    "online['CohortIndex'] = years_diff * 365 + months_diff * 30 + days_diff + 1\n",
    "print(online.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410c0ddf",
   "metadata": {},
   "source": [
    "### Cohort metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df769c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate retention rate from scratch\n",
    "\n",
    "grouping = online.groupby(['CohortMonth', 'CohortIndex'])\n",
    "\n",
    "# Count the number of unique values per customer ID\n",
    "cohort_data = grouping['CustomerID'].apply(pd.Series.nunique).reset_index()\n",
    "\n",
    "# Create a pivot \n",
    "cohort_counts = cohort_data.pivot(index='CohortMonth', columns='CohortIndex', values='CustomerID')\n",
    "\n",
    "# Select the first column and store it to cohort_sizes\n",
    "cohort_sizes = cohort_counts.iloc[:,0]\n",
    "\n",
    "# Divide the cohort count by cohort sizes along the rows\n",
    "retention = cohort_counts.divide(cohort_sizes, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1985e148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average price\n",
    "\n",
    "# Create a groupby object and pass the monthly cohort and cohort index as a list\n",
    "grouping = online.groupby(['CohortMonth', 'CohortIndex']) \n",
    "\n",
    "# Calculate the average of the unit price column\n",
    "cohort_data = grouping['UnitPrice'].mean()\n",
    "\n",
    "# Reset the index of cohort_data\n",
    "cohort_data = cohort_data.reset_index()\n",
    "\n",
    "# Create a pivot \n",
    "average_price = cohort_data.pivot(index='CohortMonth', columns='CohortIndex', values='UnitPrice')\n",
    "print(average_price.round(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7409436",
   "metadata": {},
   "source": [
    "### Visualizing cohort analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50189101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize average quantity metric\n",
    "\n",
    "# Import seaborn package as sns\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize an 8 by 6 inches plot figure\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# Add a title\n",
    "plt.title('Average Spend by Monthly Cohorts')\n",
    "\n",
    "# Create the heatmap\n",
    "sns.heatmap(average_quantity, annot=True, cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17440da",
   "metadata": {},
   "source": [
    "## Recency, Frequency, Monetary Value analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9120f6a6",
   "metadata": {},
   "source": [
    "In this second chapter, you will learn about customer segments. Specifically, you will get exposure to recency, frequency and monetary value, create customer segments based on these concepts, and analyze your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c544e9",
   "metadata": {},
   "source": [
    "### Recency, frequency, monetary (RFM) segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd37ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate spend quartiles (q=4)\n",
    "\n",
    "# Create a spend quartile with 4 groups - a range between 1 and 5\n",
    "spend_quartile = pd.qcut(data['Spend'], q=4, labels=range(1,5))\n",
    "\n",
    "# Assign the quartile values to the Spend_Quartile column in data\n",
    "data['Spend_Quartile'] = spend_quartile\n",
    "\n",
    "# Print data with sorted Spend values\n",
    "print(data.sort_values('Spend'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3f72e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate recency deciles (q=4)\n",
    "\n",
    "# Store labels from 4 to 1 in a decreasing order\n",
    "r_labels = list(range(4, 0, -1))\n",
    "\n",
    "# Create a spend quartile with 4 groups and pass the previously created labels \n",
    "recency_quartiles = pd.qcut(data['Recency_Days'], q=4, labels=r_labels)\n",
    "\n",
    "# Assign the quartile values to the Recency_Quartile column in `data`\n",
    "data['Recency_Quartile'] = recency_quartiles \n",
    "\n",
    "# Print `data` with sorted Recency_Days values\n",
    "print(data.sort_values('Recency_Days'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eff91e1",
   "metadata": {},
   "source": [
    "### Calculating RFM metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e51a8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RFM values\n",
    "\n",
    "# Calculate Recency, Frequency and Monetary value for each customer \n",
    "datamart = online.groupby(['CustomerID']).agg({\n",
    "    'InvoiceDate': lambda x: (snapshot_date - x.max()).days,\n",
    "    'InvoiceNo': 'count',\n",
    "    'TotalSum': 'sum'})\n",
    "\n",
    "# Rename the columns \n",
    "datamart.rename(columns={'InvoiceDate': 'Recency',\n",
    "                         'InvoiceNo': 'Frequency',\n",
    "                         'TotalSum': 'MonetaryValue'}, inplace=True)\n",
    "\n",
    "# Print top 5 rows\n",
    "print(datamart.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb90de1",
   "metadata": {},
   "source": [
    "### Building RFM segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4ff7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 3 groups for recency and frequency\n",
    "\n",
    "# Create labels for Recency and Frequency\n",
    "r_labels = range(3, 0, -1); f_labels = range(1, 4)\n",
    "\n",
    "# Assign these labels to three equal percentile groups \n",
    "r_groups = pd.qcut(datamart['Recency'], q=3, labels=r_labels)\n",
    "\n",
    "# Assign these labels to three equal percentile groups \n",
    "f_groups = pd.qcut(datamart['Frequency'], q=3, labels=f_labels)\n",
    "\n",
    "# Create new columns R and F \n",
    "datamart = datamart.assign(R=r_groups.values, F=f_groups.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7751d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RFM Score\n",
    "\n",
    "# Create labels for MonetaryValue\n",
    "m_labels = range(1, 4)\n",
    "\n",
    "# Assign these labels to three equal percentile groups \n",
    "m_groups = pd.qcut(datamart['MonetaryValue'], q=3, labels=m_labels)\n",
    "\n",
    "# Create new column M\n",
    "datamart = datamart.assign(M=m_groups.values)\n",
    "\n",
    "# Calculate RFM_Score\n",
    "datamart['RFM_Score'] = datamart[['R','F','M']].sum(axis=1)\n",
    "print(datamart['RFM_Score'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9ee00e",
   "metadata": {},
   "source": [
    "### Analyzing RFM table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6284ab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating custom segments\n",
    "\n",
    "# Define rfm_level function\n",
    "def rfm_level(df):\n",
    "    if df['RFM_Score'] >= 10:\n",
    "        return 'Top'\n",
    "    elif ((df['RFM_Score'] >= 6) and (df['RFM_Score'] < 10)):\n",
    "        return 'Middle'\n",
    "    else:\n",
    "        return 'Low'\n",
    "\n",
    "# Create a new variable RFM_Level\n",
    "datamart['RFM_Level'] = datamart.apply(rfm_level, axis=1)\n",
    "\n",
    "# Print the header with top 5 rows to the console\n",
    "print(datamart.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045f2a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing custom segments\n",
    "\n",
    "# Calculate average values for each RFM_Level, and return a size of each segment \n",
    "rfm_level_agg = datamart.groupby('RFM_Level').agg({\n",
    "    'Recency': 'mean',\n",
    "    'Frequency': 'mean',\n",
    "  \n",
    "  \t# Return the size of each segment\n",
    "    'MonetaryValue': ['mean', 'count']\n",
    "}).round(1)\n",
    "\n",
    "# Print the aggregated dataset\n",
    "print(rfm_level_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccc95c3",
   "metadata": {},
   "source": [
    "## Data pre-processing for clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaf0e6e",
   "metadata": {},
   "source": [
    "Once you created some segments, you want to make predictions. However, you first need to master practical data preparation methods to ensure your k-means clustering algorithm will uncover well-separated, sensible segments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6ed546",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f448ccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics of variables\n",
    "\n",
    "# Print the average values of the variables in the dataset\n",
    "print(data.mean())\n",
    "\n",
    "# Print the standard deviation of the variables in the dataset\n",
    "print(data.std())\n",
    "\n",
    "# Get the key statistics of the dataset\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7520d855",
   "metadata": {},
   "source": [
    "### Managing skewed variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd740db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect skewed variables\n",
    "\n",
    "# Plot distribution of var1\n",
    "plt.subplot(3, 1, 1); sns.distplot(data['var1'])\n",
    "\n",
    "# Plot distribution of var2\n",
    "plt.subplot(3, 1, 2); sns.distplot(data['var2'])\n",
    "\n",
    "# Plot distribution of var3\n",
    "plt.subplot(3, 1, 3); sns.distplot(data['var3'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95626ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manage skewness\n",
    "\n",
    "# Apply log transformation to var2\n",
    "data['var2_log'] = np.log(data['var2'])\n",
    "\n",
    "# Apply log transformation to var3\n",
    "data['var3_log'] = np.log(data['var3'])\n",
    "\n",
    "# Create a subplot of the distribution of var2_log\n",
    "plt.subplot(2, 1, 1); sns.distplot(data['var2_log'])\n",
    "\n",
    "# Create a subplot of the distribution of var3_log\n",
    "plt.subplot(2, 1, 2); sns.distplot(data['var3_log'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c900a38",
   "metadata": {},
   "source": [
    "### Centering and scaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bea7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center and scale manually\n",
    "\n",
    "# Center the data by subtracting average values from each entry\n",
    "data_centered = data - data.mean()\n",
    "\n",
    "# Scale the data by dividing each entry by standard deviation\n",
    "data_scaled = data / data.std()\n",
    "\n",
    "# Normalize the data by applying both centering and scaling\n",
    "data_normalized = (data - data.mean()) / data.std()\n",
    "\n",
    "# Print summary statistics to make sure average is zero and standard deviation is one\n",
    "print(data_normalized.describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99923eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center and scale with StandardScaler()\n",
    "\n",
    "# Initialize a scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "scaler.fit(data)\n",
    "\n",
    "# Scale and center the data\n",
    "data_normalized = scaler.transform(data)\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "data_normalized = pd.DataFrame(data_normalized, index=data.index, columns=data.columns)\n",
    "\n",
    "# Print summary statistics\n",
    "print(data_normalized.describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a9bade",
   "metadata": {},
   "source": [
    "### Pre-processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf1703d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize RFM distributions\n",
    "\n",
    "# Plot recency distribution\n",
    "plt.subplot(3, 1, 1); sns.distplot(datamart_rfm['Recency'])\n",
    "\n",
    "# Plot frequency distribution\n",
    "plt.subplot(3, 1, 2); sns.distplot(datamart_rfm['Frequency'])\n",
    "\n",
    "# Plot monetary value distribution\n",
    "plt.subplot(3, 1, 3); sns.distplot(datamart_rfm['MonetaryValue'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92517287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process RFM data\n",
    "\n",
    "# Unskew the data\n",
    "datamart_log = np.log(datamart_rfm)\n",
    "\n",
    "# Initialize a standard scaler and fit it\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(datamart_log)\n",
    "\n",
    "# Scale and center the data\n",
    "datamart_normalized = scaler.transform(datamart_log)\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "datamart_normalized = pd.DataFrame(data=datamart_normalized, index=datamart_rfm.index, columns=datamart_rfm.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396e7d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the normalized variables\n",
    "\n",
    "# Plot recency distribution\n",
    "plt.subplot(3, 1, 1); sns.distplot(datamart_normalized['Recency'])\n",
    "\n",
    "# Plot frequency distribution\n",
    "plt.subplot(3, 1, 2); sns.distplot(datamart_normalized['Frequency'])\n",
    "\n",
    "# Plot monetary value distribution\n",
    "plt.subplot(3, 1, 3); sns.distplot(datamart_normalized['MonetaryValue'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8049197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://campus.datacamp.com/courses/customer-segmentation-in-python/customer-segmentation-with-k-means?ex=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c3f0ed",
   "metadata": {},
   "source": [
    "## Customer Segmentation with K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a844850e",
   "metadata": {},
   "source": [
    "In this final chapter, you will use the data you pre-processed in Chapter 3 to identify customer clusters based on their recency, frequency, and monetary value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2483be",
   "metadata": {},
   "source": [
    "### Practical implementation of k-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e498e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run k-means\n",
    "\n",
    "# Import KMeans \n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Initialize KMeans\n",
    "kmeans = KMeans(n_clusters=3, random_state=1) \n",
    "\n",
    "# Fit k-means clustering on the normalized data set\n",
    "kmeans.fit(datamart_normalized)\n",
    "\n",
    "# Extract cluster labels\n",
    "cluster_labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f01802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign labels to raw data\n",
    "\n",
    "# Create a DataFrame by adding a new cluster label column\n",
    "datamart_rfm_k3 = datamart_rfm.assign(Cluster=cluster_labels)\n",
    "\n",
    "# Group the data by cluster\n",
    "grouped = datamart_rfm_k3.groupby(['Cluster'])\n",
    "\n",
    "# Calculate average RFM values and segment sizes per cluster value\n",
    "grouped.agg({\n",
    "    'Recency': 'mean',\n",
    "    'Frequency': 'mean',\n",
    "    'MonetaryValue': ['mean', 'count']\n",
    "  }).round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb92cd41",
   "metadata": {},
   "source": [
    "### Choosing the number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7201ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sum of squared errors\n",
    "\n",
    "Calculate sum of squared errors\n",
    "# Fit KMeans and calculate SSE for each k\n",
    "for k in range(1, 21):\n",
    "  \n",
    "    # Initialize KMeans with k clusters\n",
    "    kmeans = KMeans(n_clusters=k, random_state=1)\n",
    "    \n",
    "    # Fit KMeans on the normalized dataset\n",
    "    kmeans.fit(data_normalized)\n",
    "    \n",
    "    # Assign sum of squared distances to k element of dictionary\n",
    "    sse[k] = kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a51b437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sum of squared errors\n",
    "\n",
    "# Add the plot title \"The Elbow Method\"\n",
    "plt.title('The Elbow Method')\n",
    "\n",
    "# Add X-axis label \"k\"\n",
    "plt.xlabel('k')\n",
    "\n",
    "# Add Y-axis label \"SSE\"\n",
    "plt.ylabel('SSE')\n",
    "\n",
    "# Plot SSE values for each key in the dictionary\n",
    "sns.pointplot(x=list(sse.keys()), y=list(sse.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908f4e04",
   "metadata": {},
   "source": [
    "### Profile and interpret segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f2d8f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for the snake plot\n",
    "\n",
    "# Melt the normalized dataset and reset the index\n",
    "datamart_melt = pd.melt(\n",
    "  \t\t\t\t\tdatamart_normalized.reset_index(), \n",
    "                        \n",
    "# Assign CustomerID and Cluster as ID variables\n",
    "                    id_vars=['CustomerID', 'Cluster'],\n",
    "\n",
    "# Assign RFM values as value variables\n",
    "                    value_vars=['Recency', 'Frequency', 'MonetaryValue'], \n",
    "                        \n",
    "# Name the variable and value\n",
    "                    var_name='Metric', value_name='Value'\n",
    "\t\t\t\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a805c611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize snake plot\n",
    "\n",
    "# Add the plot title\n",
    "plt.title('Snake plot of normalized variables')\n",
    "\n",
    "# Add the x axis label\n",
    "plt.xlabel('Metric')\n",
    "\n",
    "# Add the y axis label\n",
    "plt.ylabel('Value')\n",
    "\n",
    "# Plot a line for each value of the cluster variable\n",
    "sns.lineplot(data=datamart_melt, x='Metric', y='Value', hue='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36056c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate relative importance of each attribute\n",
    "\n",
    "# Calculate average RFM values for each cluster\n",
    "cluster_avg = datamart_rfm_k3.groupby(['Cluster']).mean() \n",
    "\n",
    "# Calculate average RFM values for the total customer population\n",
    "population_avg = datamart_rfm.mean()\n",
    "\n",
    "# Calculate relative importance of cluster's attribute value compared to population\n",
    "relative_imp = cluster_avg / population_avg - 1\n",
    "\n",
    "# Print relative importance scores rounded to 2 decimals\n",
    "print(relative_imp.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83065d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot relative importance heatmap\n",
    "\n",
    "# Initialize a plot with a figure size of 8 by 2 inches \n",
    "plt.figure(figsize=(8, 2))\n",
    "\n",
    "# Add the plot title\n",
    "plt.title('Relative importance of attributes')\n",
    "\n",
    "# Plot the heatmap\n",
    "sns.heatmap(data=relative_imp, annot=True, fmt='.2f', cmap='RdYlGn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd0b826",
   "metadata": {},
   "source": [
    "### End-to-end segmentation solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5990349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process data\n",
    "\n",
    "# Import StandardScaler \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Apply log transformation\n",
    "datamart_rfmt_log = np.log(datamart_rfmt)\n",
    "\n",
    "# Initialize StandardScaler and fit it \n",
    "scaler = StandardScaler(); scaler.fit(datamart_rfmt_log)\n",
    "\n",
    "# Transform and store the scaled data as datamart_rfmt_normalized\n",
    "datamart_rfmt_normalized = scaler.transform(datamart_rfmt_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70d9c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot sum of squared errors\n",
    "\n",
    "# Fit KMeans and calculate SSE for each k between 1 and 10\n",
    "for k in range(1, 11):\n",
    "  \n",
    "    # Initialize KMeans with k clusters and fit it \n",
    "    kmeans = KMeans(n_clusters=k, random_state=1).fit(datamart_rfmt_normalized)\n",
    "    \n",
    "    # Assign sum of squared distances to k element of the sse dictionary\n",
    "    sse[k] = kmeans.inertia_   \n",
    "\n",
    "# Add the plot title, x and y axis labels\n",
    "plt.title('The Elbow Method'); plt.xlabel('k'); plt.ylabel('SSE')\n",
    "\n",
    "# Plot SSE values for each k stored as keys in the dictionary\n",
    "sns.pointplot(x=list(sse.keys()), y=list(sse.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ed6fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build 4-cluster solution\n",
    "\n",
    "# Import KMeans \n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Initialize KMeans\n",
    "kmeans = KMeans(n_clusters = 4, random_state = 1) \n",
    "\n",
    "# Fit k-means clustering on the normalized data set\n",
    "kmeans.fit(datamart_rfmt_normalized)\n",
    "\n",
    "# Extract cluster labels\n",
    "cluster_labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84968afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the segments\n",
    "\n",
    "# Create a new DataFrame by adding a cluster label column to datamart_rfmt\n",
    "datamart_rfmt_k4 = datamart_rfmt.assign(Cluster=cluster_labels)\n",
    "\n",
    "# Group by cluster\n",
    "grouped = datamart_rfmt_k4.groupby(['Cluster'])\n",
    "\n",
    "# Calculate average RFMT values and segment sizes for each cluster\n",
    "grouped.agg({\n",
    "    'Recency': 'mean',\n",
    "    'Frequency': 'mean',\n",
    "    'MonetaryValue': 'mean',\n",
    "    'Tenure': ['mean', 'count']\n",
    "  }).round(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
